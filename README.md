# kubernetes

# Tech with Nana - https://www.youtube.com/watch?v=X48VuDVv0do

# Kubernetes
- Open source container orchestration tool, developed by Google.
- Manages containerized applications in all env - **Physical, Virtual or Cloud and even Hybrid**
- Changes from **Monolith to Microservices** caused rise of **managing containerized application**.
- what is advantage
    - High Availability or no downtime.
    - Scalability or High performance.
    - Disater recovery - backup and restore.
    - 
# POD
- small unit in K8s.
- abstraction over a container.
- one **container/application/IP** per POD.
- new IP , every time a POD created.
- **multiple PODs** per Node.
- 
# Service & Ingress
- one service per POD.
- each service will have **one permanet IP and act as load balancer**.
- **life cycle of service & POD are not connected.**
- services are also inside the node.
- we need external service to access our application for example - in the browser.
- ingress is an external service. Request go though **(ingress service)** to **(internal service)** to **(Pod)** to **(Container)** to **(application inside container)**
- 
# Config map & Secrets - External configuration to the Pod ( Application )
- our application wants to connect with mongo-db service.
- external configuration of our application.
- database URL is kept in this config map.
- if name of the service or endpoint changes, just need to update the config map.
- **username & password** is kept inside **secrets(base 64 encoded)**, this is another type of config map.
- both config map & secrets are **configured with the POD**.
- can be used as **environment variables** or even as **properties file**.
- 
# data storage - volumes
- **kubernetes does not manage this data storage.** We need to explicity manage it.
- if the POD restarted, then data will be lost.
- **attaching volumes to the POD**. It can be **local to the node** or **remote(outside K8s cluster) to the node**
- 
# Replication of nodes
- deployments
    - for stateless applications.
    - **blue print for pods of my application**
    - Pod is abstraction on the container. **Deployment is another layer of abstraction over the pod.**
- statefulset
    - ideally we don't do this because we keep **databases outside of the K8s cluster.**
    - for statefull applications ( mysql, postgress etc )
    - databases cannnot be replicated using deployments. Reason - databases has state ( storage outside the node )
    - Need to manage, which pod is writing to the storage and reading.

# Development Tools - minicube
- minukube is a one node cluster ( acts as master as well as worker node too).
- As it as a master & worker node so **all the components of master & worker node already be installed.**
- **Minikube create virtual box on my machine.**
- **Basically minikube is a one node cluster inside a virtual box**.
- kubectl interacts with k8s cluster.
- follow https://aistudio.google.com/prompts/1sBCsV5KUDyObaOtrX7EWHL7mMlYqF7OI
- install & run docker desktop - https://www.docker.com/products/docker-desktop/
- download and run minikube exe - https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download
- minikube start --driver=docker or minikube start
- minikube delete --all -> in case of conflict with given driver with th existing driver.
- minikube status
- After success, we get message  = Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
# Development Tools - kubeadm
 - settings -> kubernetes -> enable kubernetes
 - select **kubeadm (Single Node Cluster)**. Apply & Restart.
 - **One service & one node will be already installed**.
 - kubectl get nodes/services
   - docker-desktop, control-plane (master node)
   - kubernetes ,  ClusterIP
 - **kubelet** runs on **each node**
 - **kubectl** is the command-line tool for **interacting with the Kubernetes cluster**. 

# Node
- a server ( Physical or VM) on which k8s is installed.
- if this node failes, application will be down. so k8s cluster comes in picture.
- k8s cluster is a set of nodes grouped together.
- controlplane( master node) help to manages these working nodes.
- a cluster will have master & worker nodes together.
- master node
  - api server - acts as front end for k8s.
  - etcd - disributed key-value store. All the state of a pod is stored here.
  - controller-manager - detects state changes in cluster ( Pods destroyed )
  - kube-scheduler - distributing pods to be created across multiple nodes.
- master node
  - kublet
  - container runtime ( **containerd or CRI-O**)
  - kubeproxy
 
# Master Node - Control Plane
- api-server
- etdc
- control manager
- scheduler
- api-server
  - gatway to the cluster for users & other components.

# Worker node
  - kublet
  - kubeproxy
  - container runtime ( **containerd or CRI-O**)
    - Now, **the kubelet only communicates with runtimes that directly implement the CRI standard** [ The CRI-Compliant Era (Kubernetes 1.24+) ]
    - The two most popular CRI-compliant runtimes today are:
    - **containerd:**
       - high-level container runtime that Docker itself uses under the hood! When you install Docker, you are also installing containerd.
       - Kubernetes can now bypass the Docker daemon and talk directly to containerd.
    - **CRI-O:**
       - Container Runtime Interface
       - lightweight container runtime built specifically for Kubernetes. It implements the CRI and nothing more.
       - 
 - **KUBELET**
    - is the main agent on worker node of a k8s cluster.
        - listens for instructions from the control plane - **The Lister**.
        - does the work of running and managing containers - **The Doer**.
        - monitors their health - **The Docker - The Monitor**.
        - reports the node's status back - **The Reporter**
        - provides required persistent volumes to the POD. - **The Provider or The Supplier**
    - **PODs life cycle management - The "Doer"**
       - manages entire life cycle of the PODs of the node.
       - recieves PODSpecs ( specifications for PODs ) from api-server and instruct container runtime to work on it.
       - instruct container runtime (containerd or CRI-O)
           - to pull image
           - to create container from the image.
           - to stop & remove container only if the corresponding POD is deleted or terminated.
    - **Executing health probes & monitoring of the containers of the POD - The "Doctor"**
       - continuously running **health checks defined within POD's specification**
       - Liveniness probe - check if container still running.
       - Readiness probe - check if container ready to accept traffic.
       - Startup probe - check if containerized application is started or not.
       - helps to achieve **Kubernetes' self-healing and high-availability features.**
    - **Node & POD Status reporting - The "Reporter"**
       - constantly communicates with api-server.
       - share node status ( available memory, disk space, cpu capacity ) to api-server.
       - share POD status ( pending, running, succeeded & failed PODs) to api server.
       - **it helps scheduler to function effectively. Without accurate node status, the scheduler might place PODs on unhealthy or overloaded nodes**.
    - **Managing Volumes and Secrets (The "Supplier")**
       - Mounting Volumes:
          - For volumes like ConfigMaps, Secrets, or emptyDir, it mounts them into the container.
       - Persistent Storage:
          - The kubelet now knows its mission: **"I must make the storage described by this PV available to the container(s) in this POD."**
          - "POD needs storage" request-------->into-------> providing that storage to a container on its specific node.
          - **PersistentVolume (PV):** storage that exists on (e.g., an Amazon EBS volume, a Google Persistent Disk, or an NFS share).
          - **PersistentVolumeClaim (PVC):** A POD's request for storage. **PODSpec** ->>>> **volume** section ->>>>> points to -->>>>>a **PersistentVolumeClaim**
          - **Container Storage Interface (CSI) Driver:**
             - **The CSI driver typically runs on every worker node.** .
             - Kublet call this CSI driver to talk to a PV to provide required PVC.
             - Kubelet-->>> call CSI driver- -->>> to talk to ---->> PV ----->>> to provide ---->>> required PVC for the POD.
             - The volume is now mounted to **a temporary, kubelet-managed path on the node (Node's file system)**.
             - The kubelet instructs the container runtime (*containerd or CRI-O*).
             - It says: "Start this container, and when you do, take the directory **/var/lib/kubelet/.../mount** from the host node and make it appear inside the container at the path **/data.**
             - 
- **CONATINER RUNTIME ( docker or containerd)**
    - kubelet decides what to do, the container runtime is the one that does it.
    - Running and Managing Containers
    - Managing Images
    - Managing Container Storage and Networking
    - POD Sandbox Management
       - A POD is a group of one or more containers that share a network and storage environment. The runtime is responsible for creating this shared environment         - 
 - **NETWORK PROXY**
    - runs on each node.
    - defined network rules on the node.
    - handles service discovery, load balancing of the nodes.
    - **Kube-Proxy:**
       - Service Discovery and Internal Load Balancing
          - This proxy resolves **service name to IP** of one of the healthy PODs.
          - This proxy load balances the traffic across all available PODs for that Service.
    - **Ingress Controller:**
       - A specialized proxy (like NGINX, HAProxy, or Traefik) that manages external access to the cluster.
       - This role acts as the **"front door" to the cluster**, managing how external users and systems access your applications.
       - Manages external HTTP/S traffic, **routing it to internal Services.**
    - **Service Mesh Sidecar:**
       - A proxy (like Envoy or Linkerd-proxy) that runs alongside each POD to manage inter-service communication.
       - to secure the traffic between services inside the cluster.
       - This proxy intercepts all incoming and outgoing network traffic for that POD.
       - **Secure:**
          - Automatically encrypt all traffic between services using mutual TLS (mTLS), establishing a **"zero-trust" network** where identity is verified for every request.
       - **Observe:** '
          - Generate detailed metrics (request rates, error rates, latencies), distributed traces, and access logs for all traffic without any changes to the application code.

# kubeconfig - "C:\Users\mohdr\.kube\config"
- ....
- **kubectl config view** - to view kubeconfig file
- **kubectl get --raw /api/v1/namespaces/default/PODs/nginx** - value in etdc database
- https://aistudio.google.com/app/prompts/1UdrMUn0yGZZqa46FN3rq75MaZEP-070X
- current context binds "user" and "cluster"
- client = kubectl, server = api-server
- kubectl is a command-line interface (CLI) client. Its sole purpose is to take your commands, format them into standard REST API calls, and send them to the Kubernetes API server specified in your kubeconfig file. It is the active "messenger."
- Docker Desktop is not the client in this interaction. Instead, Docker Desktop is the provider or host of the Kubernetes server. It's the application that runs all the control plane components (like the API server) in a virtual machine on your local machine.
- **apiVersion:** v1
- **kind:** Config
- **clusters:**
  - name: docker-desktop
  - cluster:
    - certificate-authority-data: DATA+OMITTED - **authority**
    - server: https://kubernetes.docker.internal:6443 - **verifyer**
- **contexts:**
  - name: docker-desktop
  - context:
    - cluster: docker-desktop
    - user: docker-desktop
- **current-context:** docker-desktop
- preferences: {}
- **users:**
  - name: docker-desktop
  - user:
    - client-certificate-data: DATA+OMITTED - **public key**
    - client-key-data: DATA+OMITTED - **private key**
    - 

# Kubernetes Kodekloud https://www.youtube.com/watch?v=XuSQU5Grv1g 
- With docker, we run one instance of an application.
- But with k8s, we can run thousands of instances in a single command.
  - "kubectl run --replicas=1000 my-web-server" =========> starting 1000 instances.
  - "kubectl scale --replicas=2000 my-web-server"========> scalling up to 2000 instances.
  - Scaling UP/DOWN the infrastrucrure/instances,can be done by configuring the k8s itself.
  - "kubectl rolling-update my-web-server --image=web-serer:2"
  - "kubectl rolling-update my-web-server --rollback"
- with K8s, we are able to define expected state of our application. This state is maintained even after any faliure to these instances.
  - webserver 2 instances.
  - payment service 2 instances.
  - redis service 3 instances.
  - database service 1 instance.
# kubectl
- **kubectl get all**
    - all objects
# POD
- kubectl version
    - show version of "client" and "server" of kubectl
- kubectl --help
- kubectl get PODs/nodes/replicatsets/deployments/services
- kubectl get PODs/nodes/replicatsets/deployments/services -o wide
  - -o wide
    - to get more details.
- kubectl run my-POD --image=nginx
    - creating POD from nginx image
- POD-definition.yaml
    - apiVersion:v1---------------------------------------version
    - kind:**POD**--------------------------------------------type
    - metadata:-------------------------------------------Meta Data - POD
        - name: myapp-POD---------------------------------name of the POD
        - labels:
            - app: myapp----------------------------------group name like front-end, back-end, sales order service
            - type: front-end
   - spec:
      - **containers**:---------------------------------------List of containers
        - -name: nginx-container--------------------------first container
        - image: nginx
        - -name: nginx-container--------------------------second container
        - image: buxybox
- kubectl create -f POD-definition.yaml
- kubectl describe PODs/nodes/replicasets/deployments/services NAME_OF_OBJECT
    - details about object.
- kubectl delete PODs/nodes/replicasets/deployments/services NAME_OF_OBJECT
    - delete an object
# Replicaset
- group of 1 or more PODs
- spans across the cluster ( 1 or more worker nodes)
- Even if replicaset has 1 POD and it fails. Replicaset automatically create another POD in place of the failed one.
- Replicaset will always make sure that "desired" number of PODs are always up.
- replicaset-definition.yaml
    - apiVersion: **apps/v1**---------------------------------------version
    - kind: **ReplicaSet**--------------------------------------------type
    - metadata:-------------------------------------------Meta Data - replicaset
        - name: myapp-replicaset---------------------------------name of the replicaset
        - labels:
            - app: myapp----------------------------------group name like front-end, back-end
            - type: front-end------------------------------label of Replicaset
   - spec:
      - **template**: --------------------------------------- **Template of a POD (metadata + spec) of a POD**
          - **metadata**:----------------------------------------Meta Data - POD
            - name: myapp-POD--------------------------------name of the POD
            - labels:
              - app: myapp-----------------------------------group name like front-end, back-end
              - type: front-end-------------------------------**label of POD**
          - **spec**:
            - containers:---------------------------------------List of containers
              - -name: nginx-container--------------------------first container
              - image: nginx
              - -name: nginx-container--------------------------second container
              - image: buxybox
      - **replicas**: 3 ---------------------------------------**3 PODS always ACTIVE all the time.**
      - **selectors**:
          - matchLabels:
              - type: front-end-----------------------------------**label of Selector**
- **Level of POD is matched with label of selectors.**

# deployments
- POD(one instance) -> Replicaset (Multiple instances) -> deployments
- rolling(old version <-> new version) update of a production application.
- rolling updates, undo changes, pause & resume changes can be done by deployments.
- **deployment-definition.yaml file is same as replicaset-definition.yaml except "kind:Deployment"**
- commands are same as replicasets.
- change the deployment file and run "kubectl apply -f deployment-definition.yaml"
# services
- IP of PODs is lost if it is started again.
- kluster IP service
    - communication wetween PODs within cluster.
    - webserver tries to connect with redis server.
    - label's of service is matched with POD's label. Only those PODs are exposed to a service.
    - cluster-ip-service.yaml
        - apiVersion:**v1**---------------------------------------version
        - kind: **Service**--------------------------------------------type
        - metadata:-------------------------------------------Meta Data - Service
            - name: redis-service---------------------------------name of the Service
       - spec:
            - type: **ClusterIP**-----------------------------------------Service Type
            - ports:
                - -targetPort: 6379--------------------------**POD port**
                - port: 6379---------------------------------**Service port** - mandatory
            - selectors
                - app: myapp-------------------------------------selection of PODs
                - mame: redis-POD---------------------------------selection of PODs
- NodePort service
    - exposed ternally on node IP and its port.
    - Layer 4 (TCP/UDP)
    - Based on NodeIP:Port'
    - It opens a specific static port (e.g., 30080) on the IP address of every node in the cluster. Any traffic hitting [Any-Node-IP]:30080 is forwarded to your service.
    - nodeport-ip-service.yaml
        - apiVersion:v1---------------------------------------version
        - kind: Service--------------------------------------------type
        - metadata:-------------------------------------------Meta Data - Service
            - name: redis-service---------------------------------name of the Service
       - spec:
            - type: **NodePort**-----------------------------------------Service Type
            - ports:
                - -targetPort: 6379--------------------------**POD port**
                - port: 6379---------------------------------**Service port** - mandatory
                - **nodePort**: **30008**-------------------------------------**Node port** (30000-32767)
            - selectors
                - app: myapp-------------------------------------selection of PODs
                - mame: redis-POD---------------------------------selection of PODs
- load balancer
    - cloud provider's load balancer to our service.
- kubectl create -f cluster-ip-service.yaml
- kubectl get services
- **curl http://192.168.1.2:30008**
- it uses **Random algorithm , sessioninfinity=yes** for the purpose of **selection of PODs to forward request to**.
- **istio & LinkerD** is also used for load balancing.
- **one POD per node, multiple PODs per node, multiple PODs across multiple nodes, same service definition will work, no additional setup needed**.

 
